{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Implementing Neural Networks from Scratch",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyODGa9tOMF+TlkYdCjsrJwG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dlsun/Data402-F21/blob/main/Implementing_Neural_Networks_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bi7JThHt1qi"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yw-_Dcal4b_"
      },
      "source": [
        "# Loss Function\n",
        "\n",
        "First, we implement the loss function $L$. \n",
        "\n",
        "Implement squared error loss below. Remember that \n",
        "$$L(y, \\hat y) = (y - \\hat y)^2 $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1Cl-EfEl4LA"
      },
      "source": [
        "class LossFunction:\n",
        "\n",
        "  def __call__(self, actual, predicted):\n",
        "    \"\"\"Evaluates L(actual, predicted).\"\"\"\n",
        "    return 0\n",
        "\n",
        "  def gradient(self, actual, predicted):\n",
        "    \"\"\"Evaluates dL/d(predicted).\"\"\"\n",
        "    return 0\n",
        "\n",
        "\n",
        "class SquaredErrorLoss(LossFunction):\n",
        "\n",
        "  def __call__(self, actual, predicted):\n",
        "    # TODO: Evaluate squared error loss\n",
        "    return\n",
        "\n",
        "  def gradient(self, actual, predicted):\n",
        "    # TODO: Evaluate the derivative of squared error loss.\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gXtMAuay8x"
      },
      "source": [
        "# Activation Function\n",
        "\n",
        "Now, we implement the activation function $g$. We need a way to evaluate the function $g(z)$, as well as a way to evaluate its derivative $g'(z)$.\n",
        "\n",
        "Remember that we can write the derivative $g' = \\frac{dh}{dz}$ in two ways, in terms of the input $z$ or in terms of the output $h$. For example, if the activation function is $g(z) = e^z$, then the derivative $\\frac{dh}{dz}$ can be expressed as $e^z$ or $h$.\n",
        "\n",
        "I have implemented ReLU for you. Your job is to implement the sigmoid activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV777Q9Wt8I2"
      },
      "source": [
        "class ActivationFunction:\n",
        "\n",
        "  def __call__(self, input):\n",
        "    \"\"\"Evaluates g(z).\"\"\"\n",
        "    output = input\n",
        "    return output\n",
        "\n",
        "  def gradient_in_terms_of_input(self, input):\n",
        "    \"\"\"Evaluates dh/dz in terms of z.\"\"\"\n",
        "    return 1\n",
        "\n",
        "  def gradient_in_terms_of_output(self, output):\n",
        "    \"\"\"Evaluates dh/dz in terms of h.\"\"\"\n",
        "    return 1\n",
        "\n",
        "\n",
        "class ReLU(ActivationFunction):\n",
        "\n",
        "  def __call__(self, input):\n",
        "    \"\"\"Evaluates g(z) = max(z, 0).\"\"\"\n",
        "    output = np.maximum(input, 0)\n",
        "    return output\n",
        "\n",
        "  def gradient_in_terms_of_input(self, input):\n",
        "    \"\"\"Evaluates g'(z) = dh/dz = (1 if z > 0 else 0).\"\"\"\n",
        "    return 1 * (input > 0)\n",
        "\n",
        "  def gradient_in_terms_of_output(self, output):\n",
        "    \"\"\"Evaluates dh/dz = (1 if h > 0, 0 if h = 0).\"\"\"\n",
        "    return 1 * (output > 0)\n",
        "\n",
        "\n",
        "class Sigmoid(ActivationFunction):\n",
        "\n",
        "  def __call__(self, input):\n",
        "    \"\"\"Evaluates g(z) = max(z, 0).\"\"\"\n",
        "    output = np.exp(input) / (1 + np.exp(input))\n",
        "    return output\n",
        "\n",
        "  def gradient_in_terms_of_input(self, input):\n",
        "    \"\"\"Evaluates g'(z) = dh/dz.\"\"\"\n",
        "    # TODO: Implement dh/dz in terms of z.\n",
        "    return\n",
        "\n",
        "  def gradient_in_terms_of_output(self, output):\n",
        "    \"\"\"Evaluates dh/dz in terms of h.\"\"\"\n",
        "    # TODO: Implement dh/dz in terms of h.\n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUj39I6EeS2p"
      },
      "source": [
        "# Layer Connection\n",
        "\n",
        "Next, we implement a layer of a neural network---or more precisely, the connection between one layer and the next. The hyperparameters are:\n",
        "- `n_input`: the number of nodes in the preceding layer.  \n",
        "- `n_output`: the number of nodes in the next layer.\n",
        "- `activation_function`: the activation function to use.\n",
        "\n",
        "There are two methods you have to implement: \n",
        "- `forward`: Given the values of input layer ${\\bf h}^{(k)}$, calculate the values of output layer ${\\bf h}^{(k+1)}$.\n",
        "- `backward`: Given backpropagated error gradient $\\frac{dL}{d{\\bf h}^{(k+1)}}$, update the weights in this layer and return $\\frac{dL}{d{\\bf h}^{(k)}}$. This in turn will get passed to the `backward()` method of the previous layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXqqoeQ24Wpr"
      },
      "source": [
        "class LayerConnection:\n",
        "  \"\"\"Represents the connection between one layer and the next.\n",
        "\n",
        "  Attributes:\n",
        "    weights: An n_input x n_output matrix representing the edge weights W.\n",
        "    bias: A scalar number representing the bias (a.k.a. intercept).\n",
        "    n_input: Number of nodes in the preceding layer.\n",
        "    n_output: Number of nodes in the next layer.\n",
        "    activation_function: ActivationFunction to apply to the linear combination.\n",
        "    current_input: Saves the most recent input layer values.\n",
        "    current_output: Saves the most recent output layer values.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, n_input, n_output, activation_function=ActivationFunction()):\n",
        "    # set atributes\n",
        "    self.n_input = n_input\n",
        "    self.n_output = n_output\n",
        "    self.activation_function = activation_function\n",
        "    # initialize the weights and bias to random numbers\n",
        "    self.weights = np.random.randn(n_input, n_output)\n",
        "    self.bias = np.random.randn()\n",
        "  \n",
        "  def forward(self, input):\n",
        "    \"\"\"Given values of the input layer, calculates values of the output layer.\n",
        "\n",
        "    Args:\n",
        "      input: An array of n_input values.\n",
        "\n",
        "    Returns:\n",
        "      An array of n_output values.\n",
        "    \"\"\"\n",
        "    # Check the dimensions of the input.\n",
        "    if len(input) != self.n_input:\n",
        "      raise ValueError(f\"Input to layer must contain {self.n_input} values.\")\n",
        "    # Save the values of the input.\n",
        "    self.current_input = input\n",
        "\n",
        "    # TODO: Calculate the values of the output.\n",
        "    self.current_output = np.zeros(self.n_output)\n",
        "\n",
        "    # Check the dimensions of the output.\n",
        "    if len(self.current_output) != self.n_output:\n",
        "      raise ValueError(f\"Layer must return {self.n_output} values.\")\n",
        "    return self.current_output\n",
        "\n",
        "  def backward(self, backpropagated_error, learning_rate):\n",
        "    \"\"\"Updates weights/bias in current layer and returns backpropagated error.\n",
        "\n",
        "    When taking derivatives, you will need the most recent values of the\n",
        "    input layer. I recommend that you use the values of the input that you\n",
        "    saved in `self.current_input`.\n",
        "\n",
        "    You will also need the derivative of the activation function in terms of\n",
        "    the output. I recommend that you use the values of the output that you \n",
        "    saved in `self.current_output`.\n",
        "\n",
        "    You should be able to do this using only matrix operations. You should\n",
        "    not need any for loops.\n",
        "\n",
        "    Args:\n",
        "      backpropagated_error: The gradient dL/d(output).\n",
        "      learning_rate: The learning rate to use in updating the weights.\n",
        "\n",
        "    Returns:\n",
        "      The gradient dL/d(input).\n",
        "    \"\"\"\n",
        "    # TODO: calculate the gradient with respect to z (instead of h)\n",
        "    dL_dz = np.zeros_like(self.current_output)\n",
        "\n",
        "    # TODO: backpropagate the error to the input layer\n",
        "    new_backpropagated_error = np.zeros(self.current_input)\n",
        "\n",
        "    # TODO: calculate the gradient of the weights and update the weights\n",
        "    self.weights -= np.zeros_like(self.weights)\n",
        "\n",
        "    # TODO: calculate the gradient of the bias and update the bias\n",
        "    self.bias -= np.zeros_like(self.bias)\n",
        "\n",
        "    # return the backpropagated error with respect to the input layer\n",
        "    return new_backpropagated_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXWWmiKLl0ie"
      },
      "source": [
        "# Neural Network\n",
        "\n",
        "A neural network consists of many layer connections. The `NeuralNetwork` class below is fully implemented for you. Read it, and make sure you understand what it is doing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4AIDMVg-DHu3"
      },
      "source": [
        "class NeuralNetwork:\n",
        "  \"\"\"A fully-connected neural network.\n",
        "\n",
        "  Attributes:\n",
        "    layers: A list of LayerConnections.\n",
        "    loss_function: A LossFunction to minimize when training the network.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, layers, loss_function):\n",
        "    self.layers = layers\n",
        "    self.loss_function = loss_function\n",
        "\n",
        "  def predict(self, input):\n",
        "    \"\"\"Predict the output for the input.\n",
        "    \n",
        "    Args:\n",
        "      input: An array of values for the input layer.\n",
        "\n",
        "    Returns:\n",
        "      An array of values for the output layer.\n",
        "    \"\"\"\n",
        "    # iterate over the layers in order\n",
        "    for layer in self.layers:\n",
        "      # calculate the output\n",
        "      output = layer.forward(input)\n",
        "      # this output is the input to the next layer\n",
        "      input = output\n",
        "    return output\n",
        "\n",
        "  def train(self, input, actual_label, learning_rate=1.0):\n",
        "    \"\"\"Update the network based on a training example.\n",
        "    \n",
        "    Args:\n",
        "      input: An array of values for the input layer.\n",
        "      actual_label: The correct label for this training example.\n",
        "      learning_rate: The learning rate to use.\n",
        "    \"\"\"\n",
        "    # calculate prediction first\n",
        "    predicted_label = self.predict(input)\n",
        "    # calculate the derivative of the loss\n",
        "    error = self.loss_function.gradient(actual_label, predicted_label)\n",
        "    # iterate over the layers in reverse\n",
        "    for layer in self.layers[::-1]:\n",
        "      # backpropagate the error, one layer at a time\n",
        "      error = layer.backward(error, learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t-gCckEoxLW"
      },
      "source": [
        "# Example\n",
        "\n",
        "If you implemented the neural network above correctly, the code below should run.\n",
        "\n",
        "This is an illustration of how neural networks can learn non-linear functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUH1usqSD2Fc"
      },
      "source": [
        "# Simulate some fake data\n",
        "n = 10000\n",
        "xs = 2 * np.random.rand(n) - 1\n",
        "ys = xs ** 2 + 0.1 * np.random.randn(n)\n",
        "\n",
        "plt.plot(xs, ys, '.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFWjj2SUpaJQ"
      },
      "source": [
        "Let's fit a neural network to this data. \n",
        "\n",
        "- We only have a single predictor $x$, so the input layer only has 1 node.\n",
        "- Let's use two hidden layers:\n",
        "    - The first will have 4 nodes.\n",
        "    - The second will have 8 nodes.\n",
        "- We want to predict a single value $y$, so the output layer also only has 1 node."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pzzdBp5OpBlS"
      },
      "source": [
        "network = NeuralNetwork(\n",
        "    layers=[\n",
        "            LayerConnection(1, 4, ReLU()),\n",
        "            LayerConnection(4, 8, ReLU()),\n",
        "            LayerConnection(8, 1)  # no activation function at the output layer\n",
        "            ],\n",
        "    loss_function=SquaredErrorLoss()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "isTGiZayqTWi"
      },
      "source": [
        "Now we train this network on our data. Notice that we make 10 _epochs_ through our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ind5GpiSqMIM"
      },
      "source": [
        "for _ in range(10):\n",
        "  for x, y in zip(xs, ys):\n",
        "    network.train(np.array([x]), np.array([y]), learning_rate=.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SlsJq3QscT1"
      },
      "source": [
        "Now, let's use our network to predict on a grid of values and plot these predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0efI8l2gqnX3"
      },
      "source": [
        "x_test = np.linspace(-1, 1, num=1000)\n",
        "y_test = []\n",
        "for x in x_test:\n",
        "  y_test.append(network.predict(np.array([x]))[0])\n",
        "\n",
        "plt.plot(xs, ys, '.')\n",
        "plt.plot(x_test, y_test, '-')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9E4sTja1HdK"
      },
      "source": [
        "Can you repeat the above using the sigmoid function?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTIp-H7B1FmZ"
      },
      "source": [
        "# TODO: Repeat with the sigmoid function."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}